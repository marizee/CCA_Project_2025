\documentclass[a4paper]{article}
\input{preamble}

\begin{document}

\thispagestyle{plain}
\begin{titlepage}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.4\textwidth]{su.png}
    \end{figure}
    \vspace{1cm}

    \begin{center}
        {\LARGE PCCA}\\[0.3cm]
        \rule{\linewidth}{0.5mm} \\[0.4cm]
        {\huge \textbf{Modular integer arithmetic and SIMD \\ vectorization using Intel AVX}}\\[0.4cm]
        \rule{\linewidth}{0.5mm} \\[1cm]
        {\large 24 January 2025 - 20 May 2025}\\[3cm]

        {\Large 
            \begin{align*}
                \textsc{Damien ASSIRE}  &- 21112838 \\
                \textsc{Marie BONBOIRE} &- 21100552
            \end{align*}
        }


    \end{center}

    \vfill
\begin{flushleft}{\large
    \textbf{Supervisor:} Mr. Vincent NEIGER\footnote{\url{https://vincent.neiger.science/}} (LIP6 - PolSys)\\
    }
\end{flushleft}
\end{titlepage}
\newpage

\tableofcontents
\newpage

%\paragraph{NOTATIONS:} (to be removed from report, just for consistency)
%\begin{itemize}
%    \item $B$ the max bitsize, either $32$ or $64$,
%    \item $n$ the modulus,
%    \item $N$ the size of a vector,
%    \item $a,b$ vectors, coefficients are $a_i$, $b_i$,
%    \item $x,y$ for integers,
%    \item $p_{hi}, p_{lo}$ temp variables,
%    \item $p$ prime number,
%    \item todo: Choose between writting high, middle and low part as $\{hi,mi,lo\}$ or $\{high, middle, low\}$ but not both,
%\end{itemize}
%
%\paragraph{Color code:}
%\begin{itemize}
%    \item \textcolor{orange}{orange}: Vincent
%    \item \textcolor{blue}{blue}: todo
%\end{itemize}
%
%\newpage
\section{Preface}

Modular integer arithmetic takes part in most of the computer algebra problems and many
cryptographic algorithms. The basic arithmetic operations therefore need highly optimized implementations
that take the most of the processor features.

The goal of this project is to study the recent tools and techniques to provide state-of-the-art implementations.
We will focus on the Intel® Advanced Vector Extensions sets for the Single Instruction Multiple Data (SIMD) vectorization,
and highlight their benefits and limitations using an advanced profiling function.

The outcomes of our analysis will be applied to a form of butterfly Fast Fourier Transform.

\bigskip
Each of the studied operations have been implemented in C over the ring $\mathbb{Z}/n\mathbb{Z}$ with $n \in \mathbb{N}$
using the Fast Library for Number Theory (FLINT) library v. 20.0.0.
We have separated the case where $n$ can fit in a 32-bit word from the case in which it doesn't 
since working over 64-bit words might require an additional care to handle potential overflows.

We only present the results obtained with a modulus of more than 32 bits and less than 64 bits.

\bigskip
The source code of this project, which represents about 5300 lines of code, along with the timing measurements, 
is entirely available on GitHub\footnote{\url{https://github.com/marizee/CCA_Project_2025}}.

The repository also contains a basic README, a Makefile, a profiling function and elementary tests to check
the correctness of our implementations.

\subsection{Machines description}

To have a thorough analysis of the enhancements offered by the vector instructions, we used 4 generations
of processors that have different features (clock speed, throughputs, cache sizes, ...):


% 2023 - 4.5 GHz % mariz
% 2019 - 2.5 GHz % ppti
% 2023 - 3.3 GHz % argiope
% 2021 - 3.0 GHz % groebner

\bigskip
\begin{tikzpicture}[very thick, black]

    %coordinates
    \coordinate (O) at (1,0); % Origin
    \coordinate (F) at (12,0); %End
    \coordinate (P1) at (2,0); %ppti
    \coordinate (P2) at (6.5,0); %groebner
    \coordinate (P3) at (11,0); %mariz+argiope

    %proc
    \draw[<-,thick,color=black] ($(P1)+(0,0.2)$) -- ($(P1)+(0,1.5)$) node [above=0pt,align=center,black] 
    {Intel® xeon® Gold 6248 Processor \\ (Cascade Lake, AVX512) \\ 2.5 GHz};
    \draw[<-,thick,color=black] ($(P2)+(0,0.2)$) -- ($(P2)+(0,0.7)$) node [above=0pt,align=center,black] 
    {Intel® xeon® Gold 6354 \\ (Ice Lake, AVX512) \\ 3.0 GHz};
    \draw[<-,thick,color=black] ($(P3)+(0,0.2)$) -- ($(P3)+(0,1.5)$) node [above=0pt,align=center,black] 
    {AMD Ryzen 7 PRO 7840U \\ (Zen 4, AVX512) \\ 3.3 GHz};
    \draw[<-,thick,color=gray] ($(P3)-(0,0.6)$) -- ($(P3)-(0,1.5)$) node [below=0pt,align=center,gray] 
    {Intel(R) Core(TM) Ultra 5 125H \\ (Meteor Lake) \\ 4.5 GHz};

    %main arrow
    \draw[->] (O) -- (F);

    %ticks
    \foreach \x in {2,6.5,11}
    \draw(\x cm,3pt) -- (\x cm,-3pt);
    %labels
    \foreach \i \j in {2/2019,6.5/2021,11/2023}{
    	\draw (\i,0) node[below=3pt] {\j} ;
    }

\end{tikzpicture}

\bigskip
The processor marked in gray corresponds to a machine that does not provide AVX512 so it is only used to check the
correctness of our implementations and will not appear in the timing comparisons of this document.

\subsection{Vectorization}

\subsubsection{Single Instruction Multiple Data}

The SIMD instructions of Intel form a set of assembly instructions that simultaneously perform an operation on multiple data stored in registers
of a fixed size allowing to significantly improve the execution speed.

They are separated in families providing different operations on the basic types (int, long, float, double). We only used intrinsics from
the AVX2 and AVX512 sets.

\bigskip
All of the SIMD instructions have a name of the form:
\begin{center} 
    \texttt{<gen><prefix><op><type><size>}
\end{center}

where
\begin{center}
    \begin{minipage}{12cm}
        \begin{itemize}
            \item[\texttt{<gen>}] empty (SSE), v (AVX, AVX2, AVX512)
            \item[\texttt{<prefix>}] empty (unpacked), p (packed)
            \item[\texttt{<op>}] add, sub, mul, sl, sr, \dots
            \item[\texttt{<type>}] l (low part), h (high part), u (unsigned), s (signed), ss (signed with saturation)
            \item[\texttt{<size>}] b (byte - 8 bits), w (word - 16 bits), d (double word - 32 bits), q (quad word - 64 bits), 
            dq (double quad - 128 bits).
        \end{itemize}
    \end{minipage}
\end{center}


\begin{remark}
    The \texttt{size} part usually refers to the size of the output but in some cases, it can contain both the
    size of the input and of the output.
    
    For example, \texttt{vpmuludq} is an AVX instruction that multiplies the low unsigned 32-bit integers from 
    packed 64-bit elements and store the unsigned 64-bit results.
\end{remark}

\bigskip
Rather than directly use these instructions that would lead to writing assembly code, Intel proposes C-style functions
along with vector types that are accessible through the header \texttt{<immintrin.h>} and require the flag \texttt{-march=native} at 
compile-time.
They are described in the Intel intrinsics guide\footnote{\url{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html}}.

\bigskip
We mainly used two types of register: 
\begin{itemize}
    \item For AVX2: \texttt{\_\_m256i} which is a 256 bits register that can store up to 4 integers of 64 bits,
    \item For AVX512: \texttt{\_\_m512i} which is a 512 bits register that can store up to 8 integers of 64 bits,
\end{itemize}


\begin{example}
    The function \texttt{\_\_m256i \_mm256\_mul\_epu32(\_\_m256i a, \_\_m256i b)} performs the instruction \texttt{vpmuludq}
    on the 4 pairs of 64-bit integers stored in the registers a and b.
\end{example}

\bigskip
The enhancements one can obtain with the AVX sets depend to a great extent on the processor used.
The interactive uops.info table \footnote{\url{https://uops.info/table.html}} gives reliable values
for the latency and the throughput of an instruction allowing to have an idea of the possible speedup 
a processor can offer.

\subsubsection{Auto-vectorization}

The compiler can also use SIMD instructions when asked with the corresponding
optimization flag. In the case of \texttt{gcc}, those flags are given by the
\texttt{-O2} and \texttt{-ftree-vectorize}
flags\footnote{\url{https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html}}.
To help the compiler to vectorize the code of a loop, it is possible to
unroll it, meaning to repeat the code of the loop body in one iteration.

For the timings, it is also possible to disable the compiler's
auto-vectorization using the \texttt{-fno-tree-vectorize} for gcc.

\textcolor{blue}{
\begin{itemize}
    \item Maybe mention that the compiler can also unroll w/ \texttt{-O3}
    \item and speak of \texttt{\_\_attribute\_\_((...))} (C syntax)
\end{itemize}}

\subsection{Timings measurement}

Timings were measured using a profiling function that resembles the ones used in FLINT.
It compares the different implementations of an operation for a given bit size of the modulus.

\bigskip
We have considered three ranges of sizes for the vectors:
\begin{itemize}
    \item small sizes: less than 200, 
    \item medium sizes: between 200 and 8000,
    \item large sizes: powers of two between $2^{13}=8192$ and $2^{22}=4194304$.
\end{itemize}

In some cases, timings obtained with small sizes and large sizes are not really conclusive because of
the mutiple load and store operations, and the memory issues that can arise from the size of the caches.

\begin{remark}
    For basic operations, one can check the coherency of the timings by considering the clock speed of
    the processor used and the number of cycles it requires to complete the operation.
    
    For example, the Zen 4 processor has a clock speed of 3.3 GHz meaning it can handle about 
    3.3 billion cycles per second. Thus, considering the throughput of an operation performed on
    $N$ pairs of coefficients, we can expect a timing of about
    \[
    \dfrac{N\cdot throughput}{3.3\times 10^9}\quad \text{seconds}.
    \]

    This computation should only be used as an estimation, as it does not take in account any
    delaying factors such as the latency.
\end{remark}

\bigskip
In theory, we could expect speed up factors of about 4 from the AVX2 intrinsics and of about 8 from the AVX512 intrinsics
since it corresponds to the number of pairs of coefficients they can handle simultaneously.
We will show that it is not particularly the case in practice.


\section{Multiplication of $64$-bit integers} 

When multiplying two integers, a problem of overflow can arise since the result might be too big to be stored in a $64$-bit word. 
To circumvent this issue, one needs to first split the two integers into two words of a given size and then apply the long multiplication.

\subsection{Long multiplication} \label{mulsplit}

The long multiplication consists in computing the product of $64$-bit integers using multiplications and additions of
integers with a smaller size.

\bigskip
Given two integers $x,y$ both of at most $64$ bits, we start by splitting them into a low part of $k$ bits
and a high one of at most $64-k$ bits with $k\leq32$:

\begin{align*}
    x &= x_{hi}\cdot 2^{k} + x_{lo} \\
    y &= y_{hi}\cdot 2^{k} + y_{lo}.
\end{align*}

\bigskip
We then compute:
\begin{align*}
    r_{lo} &= x_{lo}\cdot y_{lo} \\
    r_{mi} &= x_{lo}\cdot y_{hi} + x_{hi}\cdot y_{lo} \\
    r_{hi} &= x_{hi}\cdot y_{hi}
\end{align*}
and the final result is $ab = r_{lo} + (r_{mi} \ll k) + (r_{hi} \ll 2k)$. 
This formula is mathematically correct but is not used in practice since it would not fit in a $64$ bits word. 

\begin{remark}
    \
    \begin{itemize}
        \item For a value of $k$ lower than $32$, one has to check that each of the four products computed during the long multiplication can fit in a
        $64$-bit word, otherwise it can yield a wrong result.
        \item The addition involved in the computation of $r_{mi}$ can produce an overflow if the inputs are really up to $64$ bits.
        For this reason, we restrict the size of $x$ and $y$ to $63$ bits in practice. 
    \end{itemize}
\end{remark}

\subsection{Retrieve the high and the low part of the result}

Since the product $xy$ can have more than 64 bits, but always has at most 128 bits, the output of this operation is usually given as a pair of words of at most 64 
bits such that $xy = (xy)_{hi}\cdot 2^{64} + (xy)_{lo}$.

\bigskip
The exact formulas for these elements are:
\begin{align}
    (xy)_{lo} &= (r_{hi} \ll 2\cdot k) + (r_{mi} \ll k) + r_{lo} \nonumber \\
    (xy)_{hi} &= (r_{hi} \gg (64 - (2\cdot k))) + (r_{mi} \gg (64 - k)) + c \label{carry}
\end{align}
where $c$ corresponds to the carry coming from the computation of $(xy)_{lo}$.

\begin{remark}
    If one fixes $k=32$, then the above formulas can be slightly simplified:
    \begin{itemize}
        \item the shift left operation $(r_{hi} \ll 2\cdot k)$ will always yield 0,
        \item the shift right operation $(r_{hi} \gg (64 - (2\cdot k)))$ will always yield $r_{hi}$.
    \end{itemize}
\end{remark}

\bigskip
One can use the function \texttt{umul\_ppmm} provided by FLINT which is practical because it computes the product and
directly stores the 128-bit result before splitting it.
There are no such intrinsics in the AVX2 and AVX512 families.

\bigskip
In what follows, we consider the two operations:
\begin{itemize}
    \item \texttt{mullo(a,b)}: multiply the $x$-bit integers in $a$ and $b$, producing intermediate $2x$-bit integers, 
    and store the low $x$ bits of the intermediate integers.
    \item \texttt{mulhi(a,b)}: multiply the $x$-bit integers in $a$ and $b$, producing intermediate $2x$-bit integers, 
    and store the high $x$ bits of the intermediate integers.
\end{itemize}

In the AVX2 family, one can find intrinsics for \texttt{mullo} but only for integers up to 32 bits and intrinsics for 
\texttt{mulhi} but only for integers up to 16 bits.

In the AVX512 family, one can only find intrinsics for \texttt{mullo} (up to 64 bits) but they were rather slow
until recent processors such as ones of the Zen 4 generation\footnote{\url{https://uops.info/table.html?search=vpmullq\%20zmm&cb_lat=on&cb_tp=on&cb_uops=on&cb_CLX=on&cb_ICL=on&cb_ZEN4=on&cb_measurements=on&cb_doc=on&cb_avx512=on}}.

\bigskip
\begin{table}[h!]
    \centering
    \begin{tabularx}{0.7\textwidth} { 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X | }
        \hline
        \rowcolor{myGray} 
        Processor & Cascade Lake & IceLake & Zen 4 \\
        \hline
        \cellcolor{myGray} Throughput & 1.5 & 3.0 & 1.0 \\
        \hline
    \end{tabularx}
    \caption{Throughputs of the instruction \texttt{vpmullq zmm, zmm, zmm}.}
\end{table}

Hence, we have implemented the versions of \texttt{mullo} and \texttt{mulhi} we needed for the arithmetic operations. 

\begin{remark}
    Equivalent implementations can be found: 
    \begin{itemize}
        \item in the Vector Class library\footnote{\url{https://github.com/vectorclass/version2}} for \texttt{mullo} over 64-bit integers using AVX2,
        \item in the Intel HEXL library\cite{boemer2021intelhexlacceleratinghomomorphic} for \texttt{mulhi} over 64-bit integers using AVX512.
    \end{itemize}
\end{remark}

\subsection{Modular multiplications with precomputation}

In a context of several modular multiplications where one operand $w$ and the modulus $n$ are fixed,
V. Shoup\cite{Bos_Stam_2021} introduced a step of precomputation on $w$ and $n$ that will speed up subsequent multiplications by $w \mod n$.

\bigskip
Let $B$ be the maximum bitsize of a word ($B\in \{32, 64\}$). Given $n$ and $w \in \mathbb{Z}_n$, one can compute a scaled approximation 
of $\frac{w}{n}$, which is precisely $$ w_{pre} = \biggl\lfloor\dfrac{w\cdot 2^{B}}{n} \biggr\rfloor.$$

Then, for a vector $b = (b_1,\dots, b_N)$, one can compute $(w\cdot b_i \mod n)$ for each $i\in \{1, \dots, N\}$
using the following algorithm:

\begin{algorithm}
    \caption{Shoup modular multiplication}
    \begin{algorithmic}[1]
        \Require $n < 2^B$,
        \Require $0 < w < n$ and its precomputation $0 < w_{pre} < 2^B$,
        \Require $0 \leq b_i < n \qquad i\in \{1, \dots, N\}$.
        \Ensure $(w\cdot b_i \mod n)$.

        \State Compute $p_{hi}, p_{lo}$ such that $w_{pre} \cdot b_i = p_{hi}\cdot 2^B + p_{lo}$, \Comment{1 \texttt{mulhi}}
        \State $c \gets w\cdot b_i - p_{hi}\cdot n$ \Comment{2 \texttt{mullo}}
        \If {$c \geq n$}
            \State \Return $c-n$
        \Else
            \State \Return $c$
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{remark}
    The branching at lines 3 to 7 satisfies:
    \[
    c \mod n = 
    \left\{
        \begin{array}{ll}
            c - n & \text{ if } c \geq n \\
            c & 
        \end{array}
    \right.
    = \min(c-n, c).
    \]
    Thus, using AVX512 intrinsics, one can use \texttt{\_mm512\_min\_epu64} to reduce $c$.
\end{remark}

\bigskip
The correctness of this algorithm relies on the definition of $w_{pre}$. 

\begin{proof} (Correctness of the algorithm)
First, we have that $w_{pre}= \left\lfloor\frac{w\cdot 2^B}{n}\right\rfloor $ is the quotient in the division 
of $w\cdot 2^B$ by $n$. Thus,
\[
    w\cdot 2^B = w_{pre}\cdot n + r \text{ with } 0 \leq r < n\ \Longleftrightarrow\ w_{pre} = \dfrac{w\cdot 2^B - r}{n}
\]

Then, by definition of $p_{hi}$,
\[
p_{hi} = \left\lfloor\frac{w_{pre}\cdot b_i}{2^B}\right\rfloor
= \left\lfloor\dfrac{w\cdot b_i}{n} - \dfrac{r\cdot b_i}{n\cdot 2^B} \right\rfloor.
\]

From the requirements on $r$ and $b_i$ and the previous result, we have that
\[
\left\lfloor\dfrac{w\cdot b_i}{n}\right\rfloor - 1 \leq p_{hi} \leq \left\lfloor\dfrac{w\cdot b_i}{n}\right\rfloor
\]
and this means that $p_{hi}$ is either $\left\lfloor\frac{w\cdot b_i}{n}\right\rfloor - 1$ or $\left\lfloor\frac{w\cdot b_i}{n}\right\rfloor$.


It follows that we have
\begin{align*}
\text{either } &c=w\cdot b_i - \left\lfloor\frac{w\cdot b_i}{n}\right\rfloor n + n = (w\cdot b_i \mod n)+n \\
\text{or } &c=w\cdot b_i - \left\lfloor\frac{w\cdot b_i}{n}\right\rfloor n = w\cdot b_i \mod n
\end{align*}
and the last step of the algorithm ensures that we retrieve $w\cdot b_i \mod n$.
\end{proof}

\begin{remark}
    In this particular case, one can omit the computation of the carry that appears in equation (\ref{carry})
    when performing the \texttt{mulhi} operation on $w_{pre}\cdot b_i$.

    If so, $p_{hi}$ might be off by one bit meaning that at the step 2 of the algorithm, we have 
    \[
    c = 
    \left\{
    \begin{array}{ll}
        & \ w\cdot b_i \mod n \\
        \text{or } & (w\cdot b_i \mod n) + n \\
        \text{or } & (w\cdot b_i \mod n) + 2n
    \end{array}
    \right.
    \]
    and the last case requires an additional correction to ensure that the output will be lower than $n$.

    This is the approach we used as it significantly speeds up the operation.
\end{remark}

\section{Classic arithmetic operations on vectors}

The main goal of these implementations is to get familiar with the FLINT library and the Intel intrinsics.
It allowed us to highlight the contrasts between the different ways of vectorization, and the performances
offered by the processors we used. 

\subsection{Modular reduction}

Among the basic arithmetic operations, the modular reduction is one of the most costly. 
Thus, to circumvent its high cost, one can consider delaying it as long as there is no overflow occuring.
Another way is to modify the size requirements of the manipulated integers when the reduction boils down to
only a few additions or substractions.

These methods can't be applied on every operation. They will be more detailed when used.

\subsection{Addition of two vectors}

Given two vectors $a$, $b$ of $N$ coefficients in $\mathbb{Z}/n\mathbb{Z}$, we compute
\[
a_i + b_i \mod n \qquad \forall i\in \{1,\dots,N\}.
\]


\begin{remark}
    The modular reduction is done by only performing substractions and additions. It takes advantage of
    the fact that the sum of two integers $0 \leq x,y < n$ is less than $2n$.
\end{remark}

% 510, 2010, 32768
\begin{table}[h!]
    \centering
    
    % Proc 1: ppti
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Cascade Lake}} \\
        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 1.57e-07 & 1.0x & 7.17e-07 & 1.0x & 1.35e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 1.25e-07 & 3.7x & 6.77e-07 & 2.7x & 1.31e-05 & 2.1x \\
        \hline
        \cellcolor{myGray} AVX512 & 9.34e-08 & 5.8x & 6.73e-07 & 3.1x & 1.27e-05 & 2.4x \\
        \hline
        \cellcolor{myGray} FLINT & 1.50e-07 & 4.3x & 8.10e-07 & 2.9x & 1.56e-05 & 2.3x \\
        \hline
    \end{tabular}

    % Proc 2: groebner
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Ice Lake}} \\
        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 7.55e-08 & 1.0x & 3.34e-07 & 1.0x & 1.14e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 7.74e-08 & 4.1x & 2.93e-07 & 4.5x & 9.59e-06 & 2.5x \\
        \hline
        \cellcolor{myGray} AVX512 & 1.80e-07 & 4.0x & 4.90e-07 & 4.1x & 1.36e-05 & 1.8x \\
        \hline
        \cellcolor{myGray} FLINT & 1.50e-07 & 5.5x & 3.75e-07 & 5.6x & 1.80e-05 & 2.6x \\
        \hline
    \end{tabular}

    % Proc 3: argiope
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray}
        \multicolumn{7}{|c|}{\textsc{Zen 4}} \\
        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 7.17e-08 & 1.0x & 3.66e-07 & 1.0x & 6.16e-06 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 5.27e-08 & 4.7x & 3.48e-07 & 3.4x & 6.55e-06 & 2.7x \\
        \hline
        \cellcolor{myGray} AVX512 & 5.91e-08 & 4.3x & 3.58e-07 & 3.2x & 5.97e-06 & 3.0x \\
        \hline
        \cellcolor{myGray} FLINT & 5.55e-08 & 4.9x & 3.43e-07 & 3.1x & 6.61e-06 & 3.0x \\
        \hline
    \end{tabular}
    \caption{Timings in seconds and ratios of the modular sum with a 60-bit modulus.}
\end{table}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{add-mod_argiope.png}
    \end{center}
    \caption{Timings in seconds of the modular sum with a 60-bit modulus on a Zen 4 processor.}
\end{figure}

\newpage
We observe a speedup factor that ranges between 2.1 and 4.7 over the sequential version with the AVX2 set
and between 1.8 and 5.8 with the AVX512 set.
There is no significant differences between the enhancements offered by AVX2 and by AVX512 on this operation.

For the modular sum over 64 bits integers, the speedup factor obtained by Mathemagix with AVX2 intrinsics is about 1.8
which is less than what we have measured. 
This can be explained by the evolution of the architectures as the difference of throughput of the instruction
\texttt{\_mm256\_add\_epi64} demonstrates\footnote{\url{https://uops.info/table.html?search=vpaddq&cb_lat=on&cb_tp=on&cb_uops=on&cb_HSW=on&cb_ZEN4=on&cb_measurements=on&cb_doc=on&cb_avx2=on}}.

\subsection{Multiplication of a vector by a scalar or by another vector}


\paragraph{Scalar-vector multiplication} Given a integer $w \in \mathbb{Z}/n\mathbb{Z}$ vector $b$ of $N$ coefficients in $\mathbb{Z}/n\mathbb{Z}$, we compute

\[
w \cdot b_i \mod n \qquad \forall i\in \{1,\dots,N\}.
\]

\paragraph{Element-wise vector-vector multiplication} Given two vectors $a$, $b$ of $N$ coefficients in $\mathbb{Z}/n\mathbb{Z}$, we compute
\[
a_i \cdot b_i \mod n \qquad \forall i\in \{1,\dots,N\}.
\]

\bigskip
These two operations share about the same properties and can both be implemented using the multiplication of 64-bit integers described 
in Section \ref{mulsplit}.

\begin{remark}
    None of the mentioned methods to bypass the modular reduction can be applied here. It has to be performed after each multiplication.
\end{remark}

We only show the results obtained with the scalar-vector product that was implemented using Shoup algorithm.

\begin{table}[h!]
    \centering
    
    % Proc 1: ppti
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Cascade Lake}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 5.92e-07 & 1.0x & 2.25e-06 & 1.0x & 5.58e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 7.26e-07 & 0.8x & 2.78e-06 & 0.8x & 4.88e-05 & 0.9x \\
        \hline
        \cellcolor{myGray} AVX512 & 3.52e-07 & 1.7x & 1.23e-06 & 1.8x & 2.36e-05 & 1.4x \\
        \hline
        \cellcolor{myGray} FLINT & 7.38e-07 & 1.0x & 2.82e-06 & 1.0x & 4.77e-05 & 0.9x \\
        \hline
    \end{tabular}

    % Proc 2: groebner
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Ice Lake}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 4.96e-07 & 1.0x & 1.92e-06 & 1.0x & 3.58e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 4.38e-07 & 1.1x & 1.70e-06 & 1.1x & 3.27e-05 & 0.9x \\
        \hline
        \cellcolor{myGray} AVX512 & 1.96e-07 & 2.5x & 7.44e-07 & 2.5x & 1.55e-05 & 2.1x \\
        \hline
        \cellcolor{myGray} FLINT & 4.50e-07 & 1.0x & 1.76e-06 & 1.0x & 3.36e-05 & 0.9x \\
        \hline
    \end{tabular}

    % Proc 3: argiope
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray}
        \multicolumn{7}{|c|}{\textsc{Zen 4}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 3.35e-07 & 1.0x & 1.29e-06 & 1.0x & 2.17e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 2.35e-07 & 1.4x & 9.19e-07 & 1.4x & 1.56e-05 & 1.3x \\
        \hline
        \cellcolor{myGray} AVX512 & 1.16e-07 & 2.9x & 4.55e-07 & 2.8x & 7.53e-06 & 2.8x \\
        \hline
        \cellcolor{myGray} FLINT & 2.40e-07 & 1.0x & 9.29e-07 & 1.0x & 1.59e-05 & 1.0x \\
        \hline
    \end{tabular}
    \caption{Timings in seconds and ratios of the modular scalar vector product with a 60-bit modulus.}
\end{table}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{scalar-vector-mod_argiope.png}
    \end{center}
    \caption{Timings in seconds of the modular scalar vector product with a 60-bit modulus on a Zen 4 processor.}
\end{figure}

\newpage
Notice that the sequential version is basically what is done by FLINT in the function \\
\texttt{\_nmod\_vec\_scalar\_mul\_nmod\_shoup} which is why the two implementations have about the same timings.

With the vectorization, we obtain a speedup factor up to 1.4 with the AVX2 and up to 2.9 with the AVX512.

However, Intel HEXL does not obtain any speedup over the sequential version with the AVX512-DQ set.  

\newpage
\subsection{Dot product}

Given two vectors $a,b$ of $N$ coefficients in $\mathbb{Z}/n\mathbb{Z}$, we compute
\[
\left(a_1\cdot b_1 + a_2\cdot b_2 + \dots + a_N\cdot b_N\right) \mod n = \left(\sum_{i=1}^N a_i\cdot b_i\right) \mod n.
\]

Each multiplication is performed by using the long multiplication of Section \ref{mulsplit} and then
the products are classically added together.
Like so, it requires to perform a reduction for each binary operation.

\bigskip
In fact, one can consider delaying the reduction according to the size of the modulus.
For instance, given a $k$-bit modulus, with $k < 32$, a product $a_i\cdot b_i$ fits in a word of 64 bits so
we can sum up to $2^{64 - 2k}$ terms before doing a reduction.

\begin{example}
    If $n < 2^{30}$, then $a_i\cdot b_i < 2^{60}$ for all $i$, and we can sum up to $2^4 = 16$ terms without any overflow:
    \[
    \forall i, \qquad a_i\cdot b_i < 2^{60}  
    \Longrightarrow \sum_{i=1}^{2^4} a_i\cdot b_i < \sum_{i=1}^{2^4} 2^{60} = 2^4 \cdot 2^{60} = 2^{64}.
    \]
\end{example}

\bigskip
This can also be applied with the results of long multiplications thus allowing $n$ to have a greater bit size.
In this case, we don't retrieve the high and low parts immediately, but instead
compute the sum of each $r_{lo}$, $r_{mi}$ and $r_{lo}$ computed like in section
\ref{mulsplit}. If we split at $l$ bits with $l \leq 32$, we end up with:
\begin{itemize}
    \item $r_{lo}$ of size $2 l$
    \item $r_{mi}$ of size $(k - l) + l + 1 = k$
    \item $r_{hi}$ of size $2 \left(k - l\right)$
\end{itemize}

Hence, we can sum up to
$2^{64 - \max\left(2 l, k + 1, 2 \left(k - l\right)\right)}$ terms without
overflow.

% TODO:
% - label for sec 2

\begin{remark}
    The size of the result is not a limit for the number of term we can sum.
    This result is stored in 128 bits in total, so if $k \in \{2, \dots, 64\}$ is
    the size of the terms, up to $2^{128 - 2k}$, which is higher than the
    previous value.
\end{remark}

\begin{table}[h!]
    \centering
    
    % Proc 1: ppti
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Cascade Lake}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 2.84e-07 & 1.0x & 1.11e-06 & 1.0x & 1.92e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 2.97e-07 & 1.0x & 1.12e-06 & 1.0x & 1.94e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX512 & 2.16e-07 & 1.3x & 7.74e-07 & 1.4x & 1.44e-05 & 1.3x \\
        \hline
        \cellcolor{myGray} FLINT & 3.18e-07 & 0.9x & 1.18e-06 & 0.9x & 1.98e-05 & 1.0x \\
        \hline
    \end{tabular}

    % Proc 2: groebner
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Ice Lake}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 4.20e-07 & 1.0x & 1.54e-06 & 1.0x & 2.50e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 1.72e-07 & 2.3x & 6.45e-07 & 2.4x & 1.20e-05 & 2.1x \\
        \hline
        \cellcolor{myGray} AVX512 & 1.67e-07 & 2.4x & 6.13e-07 & 2.5x & 1.30e-05 & 1.9x \\
        \hline
        \cellcolor{myGray} FLINT & 2.87e-07 & 1.4x & 9.58e-07 & 1.6x & 1.77e-05 & 1.4x \\
        \hline
    \end{tabular}

    % Proc 3: argiope
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray}
        \multicolumn{7}{|c|}{\textsc{Zen 4}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 2.70e-07 & 1.0x & 1.20e-06 & 1.0x & 1.68e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 1.20e-07 & 2.6x & 3.87e-07 & 2.6x & 5.85e-06 & 2.9x \\
        \hline
        \cellcolor{myGray} AVX512 & 1.10e-07 & 2.7x & 3.78e-07 & 2.7x & 5.46e-06 & 3.1x \\
        \hline
        \cellcolor{myGray} FLINT & 2.70e-07 & 1.3x & 8.48e-07 & 1.2x & 1.34e-05 & 1.3x \\
        \hline
    \end{tabular}
    \caption{Timings in seconds and ratios of the modular dot product with a 60-bit modulus.}
\end{table}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{dot-prod-mod_argiope.png}
    \end{center}
    \caption{Timings in seconds of the modular dot product with a 60-bit modulus on a Zen 4 processor.}
\end{figure}

\textcolor{blue}{
    \begin{itemize}
        \item timings + comments
        \item Speak a little bit more about reconstruction and mod?
    \end{itemize}
}

% TODO: Reformulate this
The timings for the dot product are %TODO
The versions with AVX are faster than FLINT's implementation. However,
we can see that the AVX512 version is not that faster than AVX2 one on
the CPU used.

\section{Butterfly Fast Fourier Transform}

The butterfly refers to an operation that takes place in Cooley-Tukey algorithm \cite{Cooley_Tukey_1965}, a common algorithm to perform 
Fast Fourier Transform (FFT), which takes as inputs: $n, w$ a modulus and a scalar of at most 64 bits, and two integers 
$x, y$ in $\mathbb{Z}_n$, and performs the in-place computation:
\[
(x,y) \mapsto (x + w\cdot y \mod n,\ x - w\cdot y \mod n).
\]

Looking at the more global picture, in the FFT algorithm such a butterfly will
be applied to a long vector of different \(x_i, y_i\) but with the same \(w\) and \(n\).


\subsection{Harvey lazy butterfly FFT}

Harvey proposes a strategy\cite{DBLP:journals/corr/abs-1205-2926} to compute the butterfly FFT based on Shoup multiplication
with the precomputation step. It allows to save some modular reductions compared to a basic implementation, if one is able to 
restrict the sizes of the modulus.

\bigskip
The algorithm below shows our implementation.

\begin{algorithm}
    \caption{Harvey lazy butterfly FFT}
    \begin{algorithmic}[1]
        \Require $n < 2^B/4$,
        \Require $0 < w < n$ and its precomputation $0 < w_{pre} < 2^B$,
        \Require $0 \leq x, y < 4n$.
        \Ensure $(x,y) \mapsto (x + w\cdot y \mod n,\ x - w\cdot y \mod n), \qquad 0 \leq x,y < 4p.$

        \If {$x \geq 2n$}
            \State $x \gets x - 2n$
        \EndIf
        \State Compute $p_{hi}, p_{lo}$ such that $w_{pre} \cdot y = p_{hi}\cdot 2^B + p_{lo}$ \Comment{1 \texttt{mulhi}}
        \State $t \gets w\cdot y - p_{hi}\cdot n$ \Comment{2 \texttt{mullo}}
        \If {$t \geq 2n$}
            \State $t \gets t - 2n$
        \EndIf
        \State $x \gets x + t$
        \State $y \gets x - t + 2n$
        \State \Return $x,y$
    \end{algorithmic}
\end{algorithm}

\begin{remark}
    A similar version exists in the Intel HEXL library. The main difference between
    the two implementations comes from our computation of $p_{hi}$ which requires an additional correction for the outputs to be
    lower than $4n$.
\end{remark}

\begin{table}[h!]
    \centering
    
    % Proc 1: ppti
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Cascade Lake}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 8.13e-07 & 1.0x & 3.18e-06 & 1.0x & 5.28e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 7.46e-07 & 1.1x & 2.88e-06 & 1.1x & 4.71e-05 & 1.1x \\
        \hline
        \cellcolor{myGray} AVX512 & 3.87e-07 & 2.1x & 1.56e-06 & 2.0x & 3.17e-05 & 1.7x \\
        \hline
    \end{tabular}

    % Proc 2: groebner
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray} 
        \multicolumn{7}{|c|}{\textsc{Ice Lake}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 7.37e-07 & 1.0x & 2.92e-06 & 1.0x & 4.77e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 5.25e-07 & 1.4x & 2.40e-06 & 1.4x & 3.38e-05 & 1.4x \\
        \hline
        \cellcolor{myGray} AVX512 & 2.24e-07 & 3.3x & 9.37e-07 & 3.1x & 1.69e-05 & 2.8x \\
        \hline
    \end{tabular}

    % Proc 3: argiope
    \begin{tabular}{|r|*{3}{c c|}}
        \hline
        \rowcolor{myGray}
        \multicolumn{7}{|c|}{\textsc{Zen 4}} \\

        \hline
        \rowcolor{myGray}
        Ver.\textbackslash N & 510 & & 2010 & & 32768 & \\
        \hline
        \cellcolor{myGray} Seq. & 5.30e-07 & 1.0x & 2.30e-06 & 1.0x & 3.39e-05 & 1.0x \\
        \hline
        \cellcolor{myGray} AVX2 & 2.87e-07 & 1.8x & 1.12e-06 & 1.8x & 1.79e-05 & 1.9x \\
        \hline
        \cellcolor{myGray} AVX512 & 1.44e-07 & 3.5x & 5.80e-07 & 3.5x & 9.12e-06 & 3.7x \\
        \hline
    \end{tabular}
    \caption{Timings in seconds and ratios of the Harvey lazy butterfly FFT with a 60-bit modulus.}
\end{table}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{lazy-butterfly_argiope.png}
    \end{center}
    \caption{Timings in seconds of the Harvey lazy butterfly FFT with a 60-bit modulus on a Zen 4 processor.}
\end{figure}

The timings show that we can reach a speedup factor of 1.9x with the AVX2 and a significant one of 3.7x with the AVX512 over the
sequential version.


\subsection{Consequences on a complete FFT implementation}

The table below shows the results one can obtain by using our implementation of the Harvey lazy butterfly
in the context of a FFT.
It has not been done by us since it requires a further knowledge and analysis of the algorithm
which is out of the scope of this project.

\bigskip
The modulus used is a 50-bit prime number that respects the requirements for this kind of FFT.
In columns, we have:
\begin{itemize}
    \item \texttt{depth}: value of the depth for the FFT tree with $2^{depth}$ points
    in an array of length $2^{depth}$,
    \item \texttt{sd\_fft}: timings in seconds of the FFT with AVX vectorization based on floating point numbers
    already implemented in FLINT (with a modulus up to 50 bits).
    \item \texttt{dft4}: timings in seconds of the FFT without AVX vectorization from the pull request
    \footnote{\url{https://github.com/flintlib/flint/pull/2107}} that implements an integer-based 
    version of small prime FFT in FLINT (with modulus up to 62 bits).    
    \item \texttt{dft4s}: timings in seconds of the radix-4 FFT with AVX512 vectorization based on integers (with a modulus up to 62 bits),
    \item \texttt{dft2s}: timings in seconds of the radix-2 FFT with AVX512 vectorization based on integers (with a modulus up to 62 bits).
\end{itemize}

%\begin{table}[h!]
%    \centering
\begin{center}
    \begin{longtable}{|r|*{4}{c|}}
        \hline
        \rowcolor{myGray}
        depth & sd\_fft & dft4 & dft4s & dft2s \\
        \hline
        \cellcolor{myGray} 3 & 1.5e-08 & 1.4e-08 & 1.5e-08 & 1.5e-08 \\
        \hline
        \cellcolor{myGray} 4 & 2.1e-08 & 3.5e-08 & 2.2e-08 & 2.4e-08 \\
        \hline
        \cellcolor{myGray} 5 & 2.7e-08 & 8.2e-08 & 5.4e-08 & 5.3e-08 \\
        \hline
        \cellcolor{myGray} 6 & 6.2e-08 & 2.0e-07 & 9.7e-08 & 1.1e-07 \\
        \hline
        \cellcolor{myGray} 7 & 1.1e-07 & 4.4e-07 & 2.3e-07 & 2.3e-07 \\
        \hline
        \cellcolor{myGray} 8 & 2.9e-07 & 1.1e-06 & 4.8e-07 & 5.2e-07 \\
        \hline
        \cellcolor{myGray} 9 & 5.6e-07 & 2.3e-06 & 1.1e-06 & 1.1e-06 \\
        \hline
        \cellcolor{myGray} 10 & 1.3e-06 & 5.4e-06 & 2.2e-06 & 2.5e-06 \\
        \hline
        \cellcolor{myGray} 11 & 2.9e-06 & 1.1e-05 & 5.0e-06 & 5.0e-06 \\
        \hline
        \cellcolor{myGray} 12 & 6.1e-06 & 2.5e-05 & 1.0e-05 & 1.1e-05 \\
        \hline
        \cellcolor{myGray} 13 & 1.3e-05 & 5.3e-05 & 2.4e-05 & 2.4e-05 \\
        \hline
        \cellcolor{myGray} 14 & 2.9e-05 & 1.2e-04 & 4.8e-05 & 5.0e-05 \\
        \hline
        \cellcolor{myGray} 15 & 6.1e-05 & 2.4e-04 & 1.0e-04 & 1.1e-04 \\
        \hline
        \cellcolor{myGray} 16 & 1.3e-04 & 5.2e-04 & 2.0e-04 & 2.2e-04 \\
        \hline
        \cellcolor{myGray} 17 & 2.7e-04 & 1.1e-03 & 4.5e-04 & 4.6e-04 \\
        \hline
        \cellcolor{myGray} 18 & 5.8e-04 & 2.4e-03 & 9.1e-04 & 9.6e-04 \\
        \hline
        \cellcolor{myGray} 19 & 1.2e-03 & 4.9e-03 & 2.0e-03 & 2.0e-03 \\
        \hline
        \cellcolor{myGray} 20 & 2.6e-03 & 1.1e-02 & 4.1e-03 & 4.3e-03 \\
        \hline
        \cellcolor{myGray} 21 & 6.0e-03 & 2.2e-02 & 9.2e-03 & 9.6e-03 \\
        \hline
        \cellcolor{myGray} 22 & 1.3e-02 & 4.8e-02 & 2.0e-02 & 2.3e-02 \\
        \hline
        \cellcolor{myGray} 23 & 2.8e-02 & 9.9e-02 & 4.2e-02 & 4.7e-02 \\
        \hline
        \cellcolor{myGray} 24 & 6.2e-02 & 2.1e-01 & 8.3e-02 & 9.8e-02 \\
        \hline
        \cellcolor{myGray} 25 & 1.3e-01 & 4.4e-01 & 1.8e-01 & 2.1e-01 \\
        \hline
    \end{longtable}
\end{center}
%\end{table}

The vectorized versions using our implementation (columns \texttt{dft4s} and \texttt{dft2s}) provide
a speedup factor up to 2.7 over the version with no vectorization (column \texttt{dft4}).

This factor is also the one obtained by Intel HEXL and we have about the same timings as above
when running their benchmark of the FFT on the Zen 4 processor.

The timings are very encouraging as we can see that they are not too far from the version using the vectorization
based on floating point numbers.

We can hope for even better results with recent generations of processors. For example, AMD Zen 5 processors
have a \texttt{mullo\_epi64} that is two times faster than the AMD Zen 4 processors.

\textcolor{blue}{comparison to hexl results (cf mail Vincent)}

\section{Conclusion}

Altogether, the results are rather conclusive and we can hope for them to be later used in FLINT. 

\bigskip
To have a more complete study, we could have looked at the case in which the modulus is a prime number
with a particular form such as the ones presented in the appendix \ref{app}. 

This is possible in the context of the FFT, and could have enhance the butterfly FFT even more.

\bigskip
Another possible improvement is to use the Integer Fused Multiply Add instructions (IFMA) that are part of
the AVX512 family. This extension provides some intrinsics with a low throughput for operations that involve
multiplications and additions but requires to lower the maximum bit size allowed to 52 which would need the
study of floating point arithmetic.


\newpage
\bibliographystyle{plain} 
\bibliography{biblio} 
\nocite{*}


\newpage
\appendix
\section{Modular reduction with special primes} \label{app}

\subsection{Mersenne numbers}

\begin{definition}
    A Mersenne number is of the form $$ M_n = 2^n-1,$$ where $n$ is an integer.

    If $n$ is prime, $M_n$ is also prime and is therefore called a Mersenne prime.
\end{definition}

From this special form, we have for any $n$,
\[
M_n = 0 \mod M_n \Longleftrightarrow 2^n = 1 \mod M_n.
\]

Thus, any integer $x < M_n^2$ can be reduced modulo $M_n$ using only one addition: 
\begin{align*}
x \mod M_n &= x_{hi}\cdot 2^{n} + x_{lo} \mod M_n \\
    &= x_{hi} + x_{lo} \mod M_n.
\end{align*}


\subsection{Generalized Mersenne primes}

Following the same idea presented with Mersenne primes, one can expect to be able to significantly simplify
the modular reduction using generalized Mersenne primes of the form:
\begin{align*}
    p &= 2^u + 2^v + 1 \\ 
    p &= 2^u - 2^v + 1 
\end{align*}
of $u+1$ bits, with $64 > u > v > 0$.

This needs a further analysis to be conclusive.

\end{document}
